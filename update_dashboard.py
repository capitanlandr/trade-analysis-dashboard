#!/usr/bin/env python3
"""
Trade Analysis Dashboard Update Script
=====================================

This script automates the entire pipeline from data fetching to dashboard deployment.

Usage:
    python update_dashboard.py [--dry-run] [--skip-git]

Options:
    --dry-run    Show what would be done without executing
    --skip-git   Run pipeline but don't commit/push to GitHub
"""

import subprocess
import shutil
import os
import sys
import argparse
import pandas as pd
from datetime import datetime
from pathlib import Path

# Configuration
PIPELINE_DIR = "pipeline"  # Directory containing all pipeline scripts
DASHBOARD_ROOT = "."       # Root of git repo (where files get copied for Vercel)

REQUIRED_FILES = [
    "league_trades_analysis_pipeline.csv",
    "team_identity_mapping.csv", 
    "3team_trades_analysis.json"
]

# JSON files that need to be copied - these are generated by Stage 7 relative to pipeline dir
# They end up in trade-analysis-dashboard-clean/dashboard/frontend/public/ when run from pipeline/
DASHBOARD_JSON_FILES = [
    ("trade-analysis-dashboard-clean/dashboard/frontend/public/api-trades.json", "dashboard/frontend/public/api-trades.json"),
    ("trade-analysis-dashboard-clean/dashboard/frontend/public/api-teams.json", "dashboard/frontend/public/api-teams.json"),
    ("trade-analysis-dashboard-clean/dashboard/frontend/public/api-stats-summary.json", "dashboard/frontend/public/api-stats-summary.json")
]

PIPELINE_STAGES = [
    ("Stage 1: Fetch Trades", "python3 stage1_fetch_trades.py"),
    ("Stage 2: Extract Assets", "python3 stage2_extract_assets.py"),
    ("Stage 3: Cache Values", "python3 stage3_cache_values.py"),
    ("Stage 4: Generate Analysis", "python3 stage4_final.py"),
    ("Stage 5: Analyze 2026 Pick Ownership", "python3 analyze_2026_pick_ownership.py"),
    ("Stage 6: Generate Playoff Bracket", "python3 generate_playoff_bracket.py"),
    ("Stage 7: Generate Dashboard JSON", "python3 scripts/generate_dashboard_json.py")
]

def run_command(cmd, description="", dry_run=False, cwd=None):
    """Run a shell command with error handling."""
    print(f"üîÑ {description}")
    if cwd:
        print(f"   Working directory: {cwd}")
    print(f"   Command: {cmd}")
    
    if dry_run:
        print("   [DRY RUN - Not executed]")
        return True
    
    try:
        result = subprocess.run(cmd, shell=True, check=True, capture_output=True, text=True, cwd=cwd)
        if result.stdout:
            print(f"   ‚úÖ {result.stdout.strip()}")
        return True
    except subprocess.CalledProcessError as e:
        print(f"   ‚ùå Error: {e}")
        if e.stderr:
            print(f"   Error details: {e.stderr}")
        return False

def check_file_exists(filepath):
    """Check if a required file exists."""
    if os.path.exists(filepath):
        size = os.path.getsize(filepath)
        print(f"   ‚úÖ {filepath} ({size:,} bytes)")
        return True
    else:
        print(f"   ‚ùå {filepath} - NOT FOUND")
        return False



def copy_files_to_dashboard(dry_run=False):
    """Copy the 3 required files from pipeline/ to git root for Vercel."""
    print(f"\nüìÅ Copying files from {PIPELINE_DIR}/ to git root...")
    
    all_copied = True
    for filename in REQUIRED_FILES:
        src = os.path.join(PIPELINE_DIR, filename)
        dst = os.path.join(DASHBOARD_ROOT, filename)
        
        if not os.path.exists(src):
            print(f"   ‚ùå Source file {src} not found!")
            all_copied = False
            continue
            
        if dry_run:
            print(f"   [DRY RUN] Would copy {src} ‚Üí {dst}")
        else:
            try:
                shutil.copy2(src, dst)
                print(f"   ‚úÖ Copied {src} ‚Üí {dst}")
            except Exception as e:
                print(f"   ‚ùå Failed to copy {src}: {e}")
                all_copied = False
    
    # Also copy the JSON files that the dashboard frontend uses
    print(f"\nüìÅ Copying dashboard JSON files...")
    for src_path, dst_path in DASHBOARD_JSON_FILES:
        src = os.path.join(PIPELINE_DIR, src_path)
        dst = os.path.join(DASHBOARD_ROOT, dst_path)
        
        if not os.path.exists(src):
            print(f"   ‚ùå Source file {src} not found!")
            all_copied = False
            continue
            
        if dry_run:
            print(f"   [DRY RUN] Would copy {src} ‚Üí {dst}")
        else:
            try:
                # Ensure destination directory exists
                os.makedirs(os.path.dirname(dst), exist_ok=True)
                shutil.copy2(src, dst)
                print(f"   ‚úÖ Copied {src} ‚Üí {dst}")
            except Exception as e:
                print(f"   ‚ùå Failed to copy {src}: {e}")
                all_copied = False
    
    return all_copied

def git_deploy(dry_run=False):
    """Commit and push changes to trigger Vercel deployment."""
    print(f"\nüöÄ Deploying to GitHub/Vercel...")
    
    # Check if there are changes (we're already in git root)
    result = subprocess.run("git status --porcelain", shell=True, capture_output=True, text=True)
    if not result.stdout.strip():
        print("   ‚ÑπÔ∏è  No changes to commit")
        return True
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    commit_msg = f"data: update dashboard data - {timestamp}"
    
    commands = [
        ("git add .", "Adding files to git"),
        (f'git commit -m "{commit_msg}"', "Committing changes"),
        ("git push origin main", "Pushing to GitHub")
    ]
    
    for cmd, desc in commands:
        if not run_command(cmd, desc, dry_run):
            return False
    
    if not dry_run:
        print("   üéâ Deployment triggered! Check Vercel dashboard for status.")
    
    return True

def main():
    parser = argparse.ArgumentParser(description="Update Trade Analysis Dashboard")
    parser.add_argument("--dry-run", action="store_true", help="Show what would be done without executing")
    parser.add_argument("--skip-git", action="store_true", help="Run pipeline but don't commit/push")
    args = parser.parse_args()
    
    print("üèà Trade Analysis Dashboard Update Script")
    print("=" * 50)
    
    if args.dry_run:
        print("üîç DRY RUN MODE - No changes will be made")
    
    # Step 1: Run Python Pipeline (Stages 1-7) from pipeline directory
    print("\nüìä Running Python Pipeline (Stages 1-7)...")
    for stage_name, command in PIPELINE_STAGES:
        if not run_command(command, stage_name, args.dry_run, cwd=PIPELINE_DIR):
            print(f"\n‚ùå Pipeline failed at: {stage_name}")
            sys.exit(1)
    
    # Step 2: Verify output files in pipeline directory
    print(f"\nüìã Checking output files in {PIPELINE_DIR}/...")
    all_files_exist = True
    for filename in REQUIRED_FILES:
        filepath = os.path.join(PIPELINE_DIR, filename)
        if not check_file_exists(filepath):
            all_files_exist = False
    
    if not all_files_exist:
        print("\n‚ùå Some required files are missing. Pipeline may have failed.")
        sys.exit(1)
    
    # Step 3: Copy files to dashboard
    if not copy_files_to_dashboard(args.dry_run):
        print("\n‚ùå Failed to copy files to dashboard directory.")
        sys.exit(1)
    
    # Step 4: Deploy (unless skipped)
    if not args.skip_git:
        if not git_deploy(args.dry_run):
            print("\n‚ùå Deployment failed.")
            sys.exit(1)
    else:
        print("\n‚è≠Ô∏è  Skipping git deployment (--skip-git flag)")
    
    # Success!
    print("\n" + "=" * 50)
    if args.dry_run:
        print("üîç DRY RUN COMPLETE - No changes were made")
        print("   Remove --dry-run flag to execute for real")
    else:
        print("üéâ DASHBOARD UPDATE COMPLETE!")
        print("   Your dashboard should update in 2-3 minutes")
        print("   Check: https://dynasuiiiianalytics.vercel.app/")
    
    return 0

if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è  Update cancelled by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Unexpected error: {e}")
        sys.exit(1)